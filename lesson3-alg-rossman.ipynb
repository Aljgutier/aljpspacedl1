{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured and time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an implementation of the third place result in the Rossman Kaggle competition as detailed in Guo/Berkhahn's [Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737).\n",
    "\n",
    "The motivation behind exploring this architecture is it's relevance to real-world application. Most data used for decision making day-to-day in industry is structured and/or time-series data. Here we explore the end-to-end process of using neural networks with practical structured data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected Net or RNN (LSTM)?\n",
    "\n",
    "https://forums.fast.ai/t/time-series-structure-data-with-lstm/11975\n",
    "\n",
    "What you’re referring to is known as “auto-regression”. That is, using earlier time periods of the dependent variable as predictors. It is indeed common in time series, but not directly doable with the kind of model used in this notebook. For that, you’d need an RNN, which is introduced next! :slight_smile: (However, it seems that in practice RNNs aren’t giving as good results as fully connected nets for this kind of data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)\n",
    "\n",
    "PATH='data/rossmann/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\t\t models\t\t\tstore.csv\t  train.csv\r\n",
      "googletrend.csv  rossmann.tgz\t\tstore_states.csv  weather.csv\r\n",
      "joined\t\t sample_submission.csv\ttest.csv\r\n",
      "joined_test\t state_names.csv\ttmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http://files.fast.ai/part2/lesson14/rossmann.tgz).\n",
    "\n",
    "For completeness, the implementation used to put them together is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csvs(dirname):\n",
    "    path = f'{PATH}{dirname}'\n",
    "    filenames=glob(f\"{PATH}/*.csv\")\n",
    "\n",
    "    wrote_header = False\n",
    "    with open(f\"{path}.csv\",\"w\") as outputfile:\n",
    "        for filename in filenames:\n",
    "            name = filename.split(\".\")[0]\n",
    "            with open(filename) as f:\n",
    "                line = f.readline()\n",
    "                if not wrote_header:\n",
    "                    wrote_header = True\n",
    "                    outputfile.write(\"file,\"+line)\n",
    "                for line in f:\n",
    "                     outputfile.write(name + \",\" + line)\n",
    "                outputfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat_csvs('googletrend')\n",
    "#concat_csvs('weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Space:\n",
    "* train: Training set provided by competition\n",
    "* store: List of stores\n",
    "* store_states: mapping of store to the German state they are in\n",
    "* List of German state names\n",
    "* googletrend: trend of certain google keywords over time, found by users to correlate well w/ given data\n",
    "* weather: weather\n",
    "* test: testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = ['train', 'store', 'store_states', 'state_names', \n",
    "               'googletrend', 'weather', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the popular data manipulation framework `pandas`. Among other things, pandas allows you to manipulate tables/data frames in python as one would in a database.\n",
    "\n",
    "We're going to go ahead and load all of our csv's as dataframes into the list `tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `head()` to get a quick look at the contents of each table:\n",
    "* train: Contains store information on a daily basis, tracks things like sales, customers, whether that day was a holdiay, etc.\n",
    "* store: general info about the store including competition, etc.\n",
    "* store_states: maps store to state it is in\n",
    "* state_names: Maps state abbreviations to names\n",
    "* googletrend: trend data for particular week/state\n",
    "* weather: weather conditions for each state\n",
    "* test: Same as training table, w/o sales and customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          5  2015-07-31   5263        555     1      1            0   \n",
       "1      2          5  2015-07-31   6064        625     1      1            0   \n",
       "2      3          5  2015-07-31   8314        821     1      1            0   \n",
       "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
       "4      5          5  2015-07-31   4822        559     1      1            0   \n",
       "\n",
       "   SchoolHoliday  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>PromoInterval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>570.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>620.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>29910.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "0      1         c          a               1270.0                        9.0   \n",
       "1      2         a          a                570.0                       11.0   \n",
       "2      3         a          a              14130.0                       12.0   \n",
       "3      4         c          c                620.0                        9.0   \n",
       "4      5         a          a              29910.0                        4.0   \n",
       "\n",
       "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "0                    2008.0       0              NaN              NaN   \n",
       "1                    2007.0       1             13.0           2010.0   \n",
       "2                    2006.0       1             14.0           2011.0   \n",
       "3                    2009.0       0              NaN              NaN   \n",
       "4                    2015.0       0              NaN              NaN   \n",
       "\n",
       "     PromoInterval  \n",
       "0              NaN  \n",
       "1  Jan,Apr,Jul,Oct  \n",
       "2  Jan,Apr,Jul,Oct  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store State\n",
       "0      1    HE\n",
       "1      2    TH\n",
       "2      3    NW\n",
       "3      4    BE\n",
       "4      5    SN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StateName</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BadenWuerttemberg</td>\n",
       "      <td>BW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brandenburg</td>\n",
       "      <td>BB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bremen</td>\n",
       "      <td>HB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           StateName State\n",
       "0  BadenWuerttemberg    BW\n",
       "1             Bayern    BY\n",
       "2             Berlin    BE\n",
       "3        Brandenburg    BB\n",
       "4             Bremen    HB"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>week</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "      <td>2012-12-02 - 2012-12-08</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "      <td>2012-12-09 - 2012-12-15</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "      <td>2012-12-16 - 2012-12-22</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "      <td>2012-12-23 - 2012-12-29</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "      <td>2012-12-30 - 2013-01-05</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                     week  trend\n",
       "0  Rossmann_DE_SN  2012-12-02 - 2012-12-08     96\n",
       "1  Rossmann_DE_SN  2012-12-09 - 2012-12-15     95\n",
       "2  Rossmann_DE_SN  2012-12-16 - 2012-12-22     91\n",
       "3  Rossmann_DE_SN  2012-12-23 - 2012-12-29     48\n",
       "4  Rossmann_DE_SN  2012-12-30 - 2013-01-05     67"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Date</th>\n",
       "      <th>Max_TemperatureC</th>\n",
       "      <th>Mean_TemperatureC</th>\n",
       "      <th>Min_TemperatureC</th>\n",
       "      <th>Dew_PointC</th>\n",
       "      <th>MeanDew_PointC</th>\n",
       "      <th>Min_DewpointC</th>\n",
       "      <th>Max_Humidity</th>\n",
       "      <th>Mean_Humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>Max_VisibilityKm</th>\n",
       "      <th>Mean_VisibilityKm</th>\n",
       "      <th>Min_VisibilitykM</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>Max_Gust_SpeedKm_h</th>\n",
       "      <th>Precipitationmm</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>Events</th>\n",
       "      <th>WindDirDegrees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.08</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file        Date  Max_TemperatureC  Mean_TemperatureC  \\\n",
       "0  NordrheinWestfalen  2013-01-01                 8                  4   \n",
       "1  NordrheinWestfalen  2013-01-02                 7                  4   \n",
       "2  NordrheinWestfalen  2013-01-03                11                  8   \n",
       "3  NordrheinWestfalen  2013-01-04                 9                  9   \n",
       "4  NordrheinWestfalen  2013-01-05                 8                  8   \n",
       "\n",
       "   Min_TemperatureC  Dew_PointC  MeanDew_PointC  Min_DewpointC  Max_Humidity  \\\n",
       "0                 2           7               5              1            94   \n",
       "1                 1           5               3              2            93   \n",
       "2                 6          10               8              4           100   \n",
       "3                 8           9               9              8           100   \n",
       "4                 7           8               7              6           100   \n",
       "\n",
       "   Mean_Humidity  ...  Max_VisibilityKm  Mean_VisibilityKm  Min_VisibilitykM  \\\n",
       "0             87  ...              31.0               12.0               4.0   \n",
       "1             85  ...              31.0               14.0              10.0   \n",
       "2             93  ...              31.0                8.0               2.0   \n",
       "3             94  ...              11.0                5.0               2.0   \n",
       "4             94  ...              10.0                6.0               3.0   \n",
       "\n",
       "   Max_Wind_SpeedKm_h  Mean_Wind_SpeedKm_h  Max_Gust_SpeedKm_h  \\\n",
       "0                  39                   26                58.0   \n",
       "1                  24                   16                 NaN   \n",
       "2                  26                   21                 NaN   \n",
       "3                  23                   14                 NaN   \n",
       "4                  16                   10                 NaN   \n",
       "\n",
       "   Precipitationmm  CloudCover  Events  WindDirDegrees  \n",
       "0             5.08         6.0    Rain             215  \n",
       "1             0.00         6.0    Rain             225  \n",
       "2             1.02         7.0    Rain             240  \n",
       "3             0.25         7.0    Rain             263  \n",
       "4             0.00         7.0    Rain             268  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Store  DayOfWeek        Date  Open  Promo StateHoliday  SchoolHoliday\n",
       "0   1      1          4  2015-09-17   1.0      1            0              0\n",
       "1   2      3          4  2015-09-17   1.0      1            0              0\n",
       "2   3      7          4  2015-09-17   1.0      1            0              0\n",
       "3   4      8          4  2015-09-17   1.0      1            0              0\n",
       "4   5      9          4  2015-09-17   1.0      1            0              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t in tables: display(t.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very representative of a typical industry dataset.\n",
    "\n",
    "The following returns summarized aggregate information to each table accross each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>1.01721e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01721e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>558.43</td>\n",
       "      <td>3.99834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5773.82</td>\n",
       "      <td>633.146</td>\n",
       "      <td>0.830107</td>\n",
       "      <td>0.381515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.178647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.909</td>\n",
       "      <td>1.99739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3849.93</td>\n",
       "      <td>464.412</td>\n",
       "      <td>0.375539</td>\n",
       "      <td>0.485759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.383056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3727</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>558</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5744</td>\n",
       "      <td>609</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>838</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7856</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1115</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41551</td>\n",
       "      <td>7388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "      <td>1017209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>1115</td>\n",
       "      <td>7</td>\n",
       "      <td>942</td>\n",
       "      <td>21734</td>\n",
       "      <td>4086</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Store    DayOfWeek         Date        Sales    Customers  \\\n",
       "count         1.01721e+06  1.01721e+06          NaN  1.01721e+06  1.01721e+06   \n",
       "mean               558.43      3.99834          NaN      5773.82      633.146   \n",
       "std               321.909      1.99739          NaN      3849.93      464.412   \n",
       "min                     1            1          NaN            0            0   \n",
       "25%                   280            2          NaN         3727          405   \n",
       "50%                   558            4          NaN         5744          609   \n",
       "75%                   838            6          NaN         7856          837   \n",
       "max                  1115            7          NaN        41551         7388   \n",
       "counts            1017209      1017209      1017209      1017209      1017209   \n",
       "uniques              1115            7          942        21734         4086   \n",
       "missing                 0            0            0            0            0   \n",
       "missing_perc           0%           0%           0%           0%           0%   \n",
       "types             numeric      numeric  categorical      numeric      numeric   \n",
       "\n",
       "                     Open        Promo StateHoliday SchoolHoliday  \n",
       "count         1.01721e+06  1.01721e+06          NaN   1.01721e+06  \n",
       "mean             0.830107     0.381515          NaN      0.178647  \n",
       "std              0.375539     0.485759          NaN      0.383056  \n",
       "min                     0            0          NaN             0  \n",
       "25%                     1            0          NaN             0  \n",
       "50%                     1            0          NaN             0  \n",
       "75%                     1            1          NaN             0  \n",
       "max                     1            1          NaN             1  \n",
       "counts            1017209      1017209      1017209       1017209  \n",
       "uniques                 2            2            4             2  \n",
       "missing                 0            0            0             0  \n",
       "missing_perc           0%           0%           0%            0%  \n",
       "types                bool         bool  categorical          bool  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>PromoInterval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1112</td>\n",
       "      <td>761</td>\n",
       "      <td>761</td>\n",
       "      <td>1115</td>\n",
       "      <td>571</td>\n",
       "      <td>571</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5404.9</td>\n",
       "      <td>7.2247</td>\n",
       "      <td>2008.67</td>\n",
       "      <td>0.512108</td>\n",
       "      <td>23.5954</td>\n",
       "      <td>2011.76</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>322.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7663.17</td>\n",
       "      <td>3.21235</td>\n",
       "      <td>6.19598</td>\n",
       "      <td>0.500078</td>\n",
       "      <td>14.142</td>\n",
       "      <td>1.67494</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>279.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>717.5</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2325</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2012</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>836.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6882.5</td>\n",
       "      <td>10</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2013</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75860</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>1115</td>\n",
       "      <td>1115</td>\n",
       "      <td>1115</td>\n",
       "      <td>1112</td>\n",
       "      <td>761</td>\n",
       "      <td>761</td>\n",
       "      <td>1115</td>\n",
       "      <td>571</td>\n",
       "      <td>571</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>1115</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>654</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "      <td>544</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>31.75%</td>\n",
       "      <td>31.75%</td>\n",
       "      <td>0%</td>\n",
       "      <td>48.79%</td>\n",
       "      <td>48.79%</td>\n",
       "      <td>48.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>categorical</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>bool</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Store    StoreType   Assortment CompetitionDistance  \\\n",
       "count            1115          NaN          NaN                1112   \n",
       "mean              558          NaN          NaN              5404.9   \n",
       "std           322.017          NaN          NaN             7663.17   \n",
       "min                 1          NaN          NaN                  20   \n",
       "25%             279.5          NaN          NaN               717.5   \n",
       "50%               558          NaN          NaN                2325   \n",
       "75%             836.5          NaN          NaN              6882.5   \n",
       "max              1115          NaN          NaN               75860   \n",
       "counts           1115         1115         1115                1112   \n",
       "uniques          1115            4            3                 654   \n",
       "missing             0            0            0                   3   \n",
       "missing_perc       0%           0%           0%               0.27%   \n",
       "types         numeric  categorical  categorical             numeric   \n",
       "\n",
       "             CompetitionOpenSinceMonth CompetitionOpenSinceYear    Promo2  \\\n",
       "count                              761                      761      1115   \n",
       "mean                            7.2247                  2008.67  0.512108   \n",
       "std                            3.21235                  6.19598  0.500078   \n",
       "min                                  1                     1900         0   \n",
       "25%                                  4                     2006         0   \n",
       "50%                                  8                     2010         1   \n",
       "75%                                 10                     2013         1   \n",
       "max                                 12                     2015         1   \n",
       "counts                             761                      761      1115   \n",
       "uniques                             12                       23         2   \n",
       "missing                            354                      354         0   \n",
       "missing_perc                    31.75%                   31.75%        0%   \n",
       "types                          numeric                  numeric      bool   \n",
       "\n",
       "             Promo2SinceWeek Promo2SinceYear PromoInterval  \n",
       "count                    571             571           NaN  \n",
       "mean                 23.5954         2011.76           NaN  \n",
       "std                   14.142         1.67494           NaN  \n",
       "min                        1            2009           NaN  \n",
       "25%                       13            2011           NaN  \n",
       "50%                       22            2012           NaN  \n",
       "75%                       37            2013           NaN  \n",
       "max                       50            2015           NaN  \n",
       "counts                   571             571           571  \n",
       "uniques                   24               7             3  \n",
       "missing                  544             544           544  \n",
       "missing_perc          48.79%          48.79%        48.79%  \n",
       "types                numeric         numeric   categorical  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>322.017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>279.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>836.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>1115</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>1115</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Store        State\n",
       "count            1115          NaN\n",
       "mean              558          NaN\n",
       "std           322.017          NaN\n",
       "min                 1          NaN\n",
       "25%             279.5          NaN\n",
       "50%               558          NaN\n",
       "75%             836.5          NaN\n",
       "max              1115          NaN\n",
       "counts           1115         1115\n",
       "uniques          1115           12\n",
       "missing             0            0\n",
       "missing_perc       0%           0%\n",
       "types         numeric  categorical"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StateName</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NordrheinWestfalen</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>unique</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       StateName   State\n",
       "count                         16      16\n",
       "unique                        16      16\n",
       "top           NordrheinWestfalen      RP\n",
       "freq                           1       1\n",
       "counts                        16      16\n",
       "uniques                       16      16\n",
       "missing                        0       0\n",
       "missing_perc                  0%      0%\n",
       "types                     unique  unique"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>week</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.8142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.6502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>categorical</td>\n",
       "      <td>categorical</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file         week    trend\n",
       "count                 NaN          NaN     2072\n",
       "mean                  NaN          NaN  63.8142\n",
       "std                   NaN          NaN  12.6502\n",
       "min                   NaN          NaN        0\n",
       "25%                   NaN          NaN       55\n",
       "50%                   NaN          NaN       64\n",
       "75%                   NaN          NaN       72\n",
       "max                   NaN          NaN      100\n",
       "counts               2072         2072     2072\n",
       "uniques                14          148       68\n",
       "missing                 0            0        0\n",
       "missing_perc           0%           0%       0%\n",
       "types         categorical  categorical  numeric"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Date</th>\n",
       "      <th>Max_TemperatureC</th>\n",
       "      <th>Mean_TemperatureC</th>\n",
       "      <th>Min_TemperatureC</th>\n",
       "      <th>Dew_PointC</th>\n",
       "      <th>MeanDew_PointC</th>\n",
       "      <th>Min_DewpointC</th>\n",
       "      <th>Max_Humidity</th>\n",
       "      <th>Mean_Humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>Max_VisibilityKm</th>\n",
       "      <th>Mean_VisibilityKm</th>\n",
       "      <th>Min_VisibilitykM</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>Max_Gust_SpeedKm_h</th>\n",
       "      <th>Precipitationmm</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>Events</th>\n",
       "      <th>WindDirDegrees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>...</td>\n",
       "      <td>15459</td>\n",
       "      <td>15459</td>\n",
       "      <td>15459</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>3604</td>\n",
       "      <td>15840</td>\n",
       "      <td>14667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.6441</td>\n",
       "      <td>10.389</td>\n",
       "      <td>6.19899</td>\n",
       "      <td>8.58782</td>\n",
       "      <td>6.20581</td>\n",
       "      <td>3.62614</td>\n",
       "      <td>93.6596</td>\n",
       "      <td>74.2829</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0576</td>\n",
       "      <td>12.2398</td>\n",
       "      <td>7.02516</td>\n",
       "      <td>22.7666</td>\n",
       "      <td>11.9722</td>\n",
       "      <td>48.8643</td>\n",
       "      <td>0.831718</td>\n",
       "      <td>5.55131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.64601</td>\n",
       "      <td>7.37926</td>\n",
       "      <td>6.52639</td>\n",
       "      <td>6.24478</td>\n",
       "      <td>6.08677</td>\n",
       "      <td>6.12839</td>\n",
       "      <td>7.67853</td>\n",
       "      <td>13.4866</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9768</td>\n",
       "      <td>5.06794</td>\n",
       "      <td>4.9806</td>\n",
       "      <td>8.98862</td>\n",
       "      <td>5.87284</td>\n",
       "      <td>13.027</td>\n",
       "      <td>2.51351</td>\n",
       "      <td>1.68771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11</td>\n",
       "      <td>-13</td>\n",
       "      <td>-15</td>\n",
       "      <td>-14</td>\n",
       "      <td>-15</td>\n",
       "      <td>-73</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>90.75</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>101</td>\n",
       "      <td>53</td>\n",
       "      <td>111</td>\n",
       "      <td>58.93</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>...</td>\n",
       "      <td>15459</td>\n",
       "      <td>15459</td>\n",
       "      <td>15459</td>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>3604</td>\n",
       "      <td>15840</td>\n",
       "      <td>14667</td>\n",
       "      <td>11889</td>\n",
       "      <td>15840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>16</td>\n",
       "      <td>990</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>381</td>\n",
       "      <td>381</td>\n",
       "      <td>381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12236</td>\n",
       "      <td>0</td>\n",
       "      <td>1173</td>\n",
       "      <td>3951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>...</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>77.25%</td>\n",
       "      <td>0%</td>\n",
       "      <td>7.41%</td>\n",
       "      <td>24.94%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>categorical</td>\n",
       "      <td>categorical</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file         Date Max_TemperatureC Mean_TemperatureC  \\\n",
       "count                 NaN          NaN            15840             15840   \n",
       "mean                  NaN          NaN          14.6441            10.389   \n",
       "std                   NaN          NaN          8.64601           7.37926   \n",
       "min                   NaN          NaN              -11               -13   \n",
       "25%                   NaN          NaN                8                 4   \n",
       "50%                   NaN          NaN               15                11   \n",
       "75%                   NaN          NaN               21                16   \n",
       "max                   NaN          NaN               39                31   \n",
       "counts              15840        15840            15840             15840   \n",
       "uniques                16          990               51                45   \n",
       "missing                 0            0                0                 0   \n",
       "missing_perc           0%           0%               0%                0%   \n",
       "types         categorical  categorical          numeric           numeric   \n",
       "\n",
       "             Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC  \\\n",
       "count                   15840      15840          15840         15840   \n",
       "mean                  6.19899    8.58782        6.20581       3.62614   \n",
       "std                   6.52639    6.24478        6.08677       6.12839   \n",
       "min                       -15        -14            -15           -73   \n",
       "25%                         1          4              2            -1   \n",
       "50%                         7          9              7             4   \n",
       "75%                        11         13             11             8   \n",
       "max                        24         25             20            19   \n",
       "counts                  15840      15840          15840         15840   \n",
       "uniques                    40         40             36            40   \n",
       "missing                     0          0              0             0   \n",
       "missing_perc               0%         0%             0%            0%   \n",
       "types                 numeric    numeric        numeric       numeric   \n",
       "\n",
       "             Max_Humidity Mean_Humidity  ... Max_VisibilityKm  \\\n",
       "count               15840         15840  ...            15459   \n",
       "mean              93.6596       74.2829  ...          24.0576   \n",
       "std               7.67853       13.4866  ...           8.9768   \n",
       "min                    44            30  ...                0   \n",
       "25%                 90.75            65  ...               14   \n",
       "50%                    94            76  ...               31   \n",
       "75%                   100            85  ...               31   \n",
       "max                   100           100  ...               31   \n",
       "counts              15840         15840  ...            15459   \n",
       "uniques                53            71  ...               24   \n",
       "missing                 0             0  ...              381   \n",
       "missing_perc           0%            0%  ...            2.41%   \n",
       "types             numeric       numeric  ...          numeric   \n",
       "\n",
       "             Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h  \\\n",
       "count                    15459            15459              15840   \n",
       "mean                   12.2398          7.02516            22.7666   \n",
       "std                    5.06794           4.9806            8.98862   \n",
       "min                          0                0                  3   \n",
       "25%                         10                3                 16   \n",
       "50%                         11                7                 21   \n",
       "75%                         14               10                 27   \n",
       "max                         31               31                101   \n",
       "counts                   15459            15459              15840   \n",
       "uniques                     32               24                 44   \n",
       "missing                    381              381                  0   \n",
       "missing_perc             2.41%            2.41%                 0%   \n",
       "types                  numeric          numeric            numeric   \n",
       "\n",
       "             Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm  \\\n",
       "count                      15840               3604           15840   \n",
       "mean                     11.9722            48.8643        0.831718   \n",
       "std                      5.87284             13.027         2.51351   \n",
       "min                            2                 21               0   \n",
       "25%                            8                 39               0   \n",
       "50%                           11                 48               0   \n",
       "75%                           14                 55            0.25   \n",
       "max                           53                111           58.93   \n",
       "counts                     15840               3604           15840   \n",
       "uniques                       29                 47              41   \n",
       "missing                        0              12236               0   \n",
       "missing_perc                  0%             77.25%              0%   \n",
       "types                    numeric            numeric         numeric   \n",
       "\n",
       "             CloudCover       Events WindDirDegrees  \n",
       "count             14667          NaN          15840  \n",
       "mean            5.55131          NaN        175.897  \n",
       "std             1.68771          NaN        101.589  \n",
       "min                   0          NaN             -1  \n",
       "25%                   5          NaN             80  \n",
       "50%                   6          NaN            202  \n",
       "75%                   7          NaN            256  \n",
       "max                   8          NaN            360  \n",
       "counts            14667        11889          15840  \n",
       "uniques               9           21            362  \n",
       "missing            1173         3951              0  \n",
       "missing_perc      7.41%       24.94%             0%  \n",
       "types           numeric  categorical        numeric  \n",
       "\n",
       "[13 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41077</td>\n",
       "      <td>41088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20544.5</td>\n",
       "      <td>555.9</td>\n",
       "      <td>3.97917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.854322</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.443487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11861.2</td>\n",
       "      <td>320.274</td>\n",
       "      <td>2.01548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352787</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10272.8</td>\n",
       "      <td>279.75</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20544.5</td>\n",
       "      <td>553.5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30816.2</td>\n",
       "      <td>832.25</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41088</td>\n",
       "      <td>1115</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>41077</td>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "      <td>41088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>41088</td>\n",
       "      <td>856</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id    Store DayOfWeek         Date      Open     Promo  \\\n",
       "count           41088    41088     41088          NaN     41077     41088   \n",
       "mean          20544.5    555.9   3.97917          NaN  0.854322  0.395833   \n",
       "std           11861.2  320.274   2.01548          NaN  0.352787  0.489035   \n",
       "min                 1        1         1          NaN         0         0   \n",
       "25%           10272.8   279.75         2          NaN         1         0   \n",
       "50%           20544.5    553.5         4          NaN         1         0   \n",
       "75%           30816.2   832.25         6          NaN         1         1   \n",
       "max             41088     1115         7          NaN         1         1   \n",
       "counts          41088    41088     41088        41088     41077     41088   \n",
       "uniques         41088      856         7           48         2         2   \n",
       "missing             0        0         0            0        11         0   \n",
       "missing_perc       0%       0%        0%           0%     0.03%        0%   \n",
       "types         numeric  numeric   numeric  categorical      bool      bool   \n",
       "\n",
       "             StateHoliday SchoolHoliday  \n",
       "count                 NaN         41088  \n",
       "mean                  NaN      0.443487  \n",
       "std                   NaN      0.496802  \n",
       "min                   NaN             0  \n",
       "25%                   NaN             0  \n",
       "50%                   NaN             0  \n",
       "75%                   NaN             1  \n",
       "max                   NaN             1  \n",
       "counts              41088         41088  \n",
       "uniques                 2             2  \n",
       "missing                 0             0  \n",
       "missing_perc           0%            0%  \n",
       "types                bool          bool  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t in tables: display(DataFrameSummary(t).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a structured data problem, we necessarily have to go through all the cleaning and feature engineering, even though we're using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, store, store_states, state_names, googletrend, weather, test = tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017209, 41088)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.StateHoliday = train.StateHoliday!='0'\n",
    "test.StateHoliday = test.StateHoliday!='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n",
    "\n",
    "Pandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df(left, right, left_on, right_on=None, suffix='_y'):\n",
    "    if right_on is None: right_on = left_on\n",
    "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
    "                      suffixes=(\"\", suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join weather/state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = join_df(weather, state_names, \"file\", \"StateName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n",
    "\n",
    "We're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\n",
    "googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n",
    "googletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n",
    "\n",
    "You should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(weather, \"Date\", drop=False)\n",
    "add_datepart(googletrend, \"Date\", drop=False)\n",
    "add_datepart(train, \"Date\", drop=False)\n",
    "add_datepart(test, \"Date\", drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google trends data has a special category for the whole of Germany - we'll pull that out so we can use it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_de = googletrend[googletrend.file == 'Rossmann_DE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n",
    "\n",
    "*Aside*: Why note just do an inner join?\n",
    "If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #'s. Outer join is easier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = join_df(store, store_states, \"Store\")\n",
    "len(store[store.State.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = join_df(train, store, \"Store\")\n",
    "joined_test = join_df(test, store, \"Store\")\n",
    "len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\n",
    "joined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\n",
    "len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
    "joined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
    "len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = join_df(joined, weather, [\"State\",\"Date\"])\n",
    "joined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\n",
    "len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (joined, joined_test):\n",
    "    for c in df.columns:\n",
    "        if c.endswith('_y'):\n",
    "            if c in df.columns: df.drop(c, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (joined,joined_test):\n",
    "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
    "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
    "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
    "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (joined,joined_test):\n",
    "    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n",
    "                                                     month=df.CompetitionOpenSinceMonth, day=15))\n",
    "    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace some erroneous / outlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (joined,joined_test):\n",
    "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
    "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24,  3, 19,  9,  0, 16, 17,  7, 15, 22, 11, 13,  2, 23, 12,  4, 10,  1, 14, 20,  8, 18,  6, 21,  5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in (joined,joined_test):\n",
    "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
    "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n",
    "joined.CompetitionMonthsOpen.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process for Promo dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://forums.fast.ai/t/rossman-data-clean-ipynb-got-typeerror-dtype-class-datetime-datetime-not-understood/38936\n",
    "# ... delete the \".astype() part\"\n",
    "for df in (joined,joined_test):\n",
    "    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n",
    "        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))\n",
    "    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (joined,joined_test):\n",
    "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
    "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
    "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
    "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
    "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
    "    df.Promo2Weeks.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.to_feather(f'{PATH}joined')\n",
    "joined_test.to_feather(f'{PATH}joined_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n",
    "* Running averages\n",
    "* Time until next event\n",
    "* Time since last event\n",
    "\n",
    "This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n",
    "\n",
    "We'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n",
    "\n",
    "Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elapsed(fld, pre):\n",
    "    day1 = np.timedelta64(1, 'D')\n",
    "    last_date = np.datetime64()\n",
    "    last_store = 0\n",
    "    res = []\n",
    "\n",
    "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
    "        if s != last_store:\n",
    "            last_date = np.datetime64()\n",
    "            last_store = s\n",
    "        if v: last_date = d\n",
    "        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n",
    "    df[pre+fld] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be applying this to a subset of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = train[columns]\n",
    "df = train[columns].append(test[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through an example.\n",
    "\n",
    "Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\n",
    "This will apply to each row with School Holiday:\n",
    "* A applied to every row of the dataframe in order of store and date\n",
    "* Will add to the dataframe the days since seeing a School Holiday\n",
    "* If we sort in the other direction, this will count the days until another holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'SchoolHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this for two more fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'StateHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'Promo'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to set the active index to Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set null values from elapsed field calculations to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['SchoolHoliday', 'StateHoliday', 'Promo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in ['Before', 'After']:\n",
    "    for p in columns:\n",
    "        a = o+p\n",
    "        df[a] = df[a].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n",
    "\n",
    "Here we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd = df[['Store']+columns].sort_index(ascending=False\n",
    "                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to drop the Store indices grouped together in the window function.\n",
    "\n",
    "Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd.drop('Store',1,inplace=True)\n",
    "bwd.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd.drop('Store',1,inplace=True)\n",
    "fwd.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge these values onto the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\n",
    "df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Store</th>\n",
       "      <th>AfterSchoolHoliday</th>\n",
       "      <th>BeforeSchoolHoliday</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>AfterPromo</th>\n",
       "      <th>BeforePromo</th>\n",
       "      <th>SchoolHoliday_bw</th>\n",
       "      <th>StateHoliday_bw</th>\n",
       "      <th>Promo_bw</th>\n",
       "      <th>SchoolHoliday_fw</th>\n",
       "      <th>StateHoliday_fw</th>\n",
       "      <th>Promo_fw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-09-13</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Store  AfterSchoolHoliday  BeforeSchoolHoliday  \\\n",
       "0 2015-09-17      1                  13                    0   \n",
       "1 2015-09-16      1                  12                    0   \n",
       "2 2015-09-15      1                  11                    0   \n",
       "3 2015-09-14      1                  10                    0   \n",
       "4 2015-09-13      1                   9                    0   \n",
       "\n",
       "   AfterStateHoliday  BeforeStateHoliday  AfterPromo  BeforePromo  \\\n",
       "0                105                   0           0            0   \n",
       "1                104                   0           0            0   \n",
       "2                103                   0           0            0   \n",
       "3                102                   0           0            0   \n",
       "4                101                   0           9           -1   \n",
       "\n",
       "   SchoolHoliday_bw  StateHoliday_bw  Promo_bw  SchoolHoliday_fw  \\\n",
       "0               0.0              0.0       4.0               0.0   \n",
       "1               0.0              0.0       3.0               0.0   \n",
       "2               0.0              0.0       2.0               0.0   \n",
       "3               0.0              0.0       1.0               0.0   \n",
       "4               0.0              0.0       0.0               0.0   \n",
       "\n",
       "   StateHoliday_fw  Promo_fw  \n",
       "0              0.0       1.0  \n",
       "1              0.0       2.0  \n",
       "2              0.0       3.0  \n",
       "3              0.0       4.0  \n",
       "4              0.0       4.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f'{PATH}df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(f'{PATH}df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Store', 'AfterSchoolHoliday', 'BeforeSchoolHoliday',\n",
       "       'AfterStateHoliday', 'BeforeStateHoliday', 'AfterPromo', 'BeforePromo',\n",
       "       'SchoolHoliday_bw', 'StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw',\n",
       "       'StateHoliday_fw', 'Promo_fw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = join_df(joined, df, ['Store', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test = join_df(joined_test, df, ['Store', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined[joined.Sales!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll back this up as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.reset_index(inplace=True)\n",
    "joined_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.to_feather(f'{PATH}joined')\n",
    "joined_test.to_feather(f'{PATH}joined_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our final set of engineered features.\n",
    "\n",
    "While these steps were explicitly outlined in the paper, these are all fairly typical feature engineering steps for dealing with time series data and are practical in any similar setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features ... load datset with features and prepare for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.read_feather(f'{PATH}joined')\n",
    "joined_test = pd.read_feather(f'{PATH}joined_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Store</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DayOfWeek</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>5263</td>\n",
       "      <td>6064</td>\n",
       "      <td>8314</td>\n",
       "      <td>13995</td>\n",
       "      <td>4822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customers</th>\n",
       "      <td>555</td>\n",
       "      <td>625</td>\n",
       "      <td>821</td>\n",
       "      <td>1498</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StateHoliday</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dayofweek</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dayofyear</th>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_month_end</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_month_start</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_quarter_end</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_quarter_start</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_year_end</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_year_start</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elapsed</th>\n",
       "      <td>1438300800</td>\n",
       "      <td>1438300800</td>\n",
       "      <td>1438300800</td>\n",
       "      <td>1438300800</td>\n",
       "      <td>1438300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreType</th>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assortment</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <td>1270</td>\n",
       "      <td>570</td>\n",
       "      <td>14130</td>\n",
       "      <td>620</td>\n",
       "      <td>29910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <td>2008</td>\n",
       "      <td>2007</td>\n",
       "      <td>2006</td>\n",
       "      <td>2009</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promo2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <td>1900</td>\n",
       "      <td>2010</td>\n",
       "      <td>2011</td>\n",
       "      <td>1900</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PromoInterval</th>\n",
       "      <td>None</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "      <td>Jan,Apr,Jul,Oct</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>HE</td>\n",
       "      <td>TH</td>\n",
       "      <td>NW</td>\n",
       "      <td>BE</td>\n",
       "      <td>SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>Rossmann_DE_HE</td>\n",
       "      <td>Rossmann_DE_TH</td>\n",
       "      <td>Rossmann_DE_NW</td>\n",
       "      <td>Rossmann_DE_BE</td>\n",
       "      <td>Rossmann_DE_SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>2015-08-02 - 2015-08-08</td>\n",
       "      <td>2015-08-02 - 2015-08-08</td>\n",
       "      <td>2015-08-02 - 2015-08-08</td>\n",
       "      <td>2015-08-02 - 2015-08-08</td>\n",
       "      <td>2015-08-02 - 2015-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trend</th>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>74</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max_TemperatureC</th>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_TemperatureC</th>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min_TemperatureC</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dew_PointC</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0                        1  \\\n",
       "index                                            0                        1   \n",
       "Store                                            1                        2   \n",
       "DayOfWeek                                        5                        5   \n",
       "Date                           2015-07-31 00:00:00      2015-07-31 00:00:00   \n",
       "Sales                                         5263                     6064   \n",
       "Customers                                      555                      625   \n",
       "Open                                             1                        1   \n",
       "Promo                                            1                        1   \n",
       "StateHoliday                                 False                    False   \n",
       "SchoolHoliday                                    1                        1   \n",
       "Year                                          2015                     2015   \n",
       "Month                                            7                        7   \n",
       "Week                                            31                       31   \n",
       "Day                                             31                       31   \n",
       "Dayofweek                                        4                        4   \n",
       "Dayofyear                                      212                      212   \n",
       "Is_month_end                                  True                     True   \n",
       "Is_month_start                               False                    False   \n",
       "Is_quarter_end                               False                    False   \n",
       "Is_quarter_start                             False                    False   \n",
       "Is_year_end                                  False                    False   \n",
       "Is_year_start                                False                    False   \n",
       "Elapsed                                 1438300800               1438300800   \n",
       "StoreType                                        c                        a   \n",
       "Assortment                                       a                        a   \n",
       "CompetitionDistance                           1270                      570   \n",
       "CompetitionOpenSinceMonth                        9                       11   \n",
       "CompetitionOpenSinceYear                      2008                     2007   \n",
       "Promo2                                           0                        1   \n",
       "Promo2SinceWeek                                  1                       13   \n",
       "Promo2SinceYear                               1900                     2010   \n",
       "PromoInterval                                 None          Jan,Apr,Jul,Oct   \n",
       "State                                           HE                       TH   \n",
       "file                                Rossmann_DE_HE           Rossmann_DE_TH   \n",
       "week                       2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
       "trend                                           85                       80   \n",
       "Max_TemperatureC                                23                       19   \n",
       "Mean_TemperatureC                               16                       13   \n",
       "Min_TemperatureC                                 8                        7   \n",
       "Dew_PointC                                       9                        9   \n",
       "\n",
       "                                                 2                        3  \\\n",
       "index                                            2                        3   \n",
       "Store                                            3                        4   \n",
       "DayOfWeek                                        5                        5   \n",
       "Date                           2015-07-31 00:00:00      2015-07-31 00:00:00   \n",
       "Sales                                         8314                    13995   \n",
       "Customers                                      821                     1498   \n",
       "Open                                             1                        1   \n",
       "Promo                                            1                        1   \n",
       "StateHoliday                                 False                    False   \n",
       "SchoolHoliday                                    1                        1   \n",
       "Year                                          2015                     2015   \n",
       "Month                                            7                        7   \n",
       "Week                                            31                       31   \n",
       "Day                                             31                       31   \n",
       "Dayofweek                                        4                        4   \n",
       "Dayofyear                                      212                      212   \n",
       "Is_month_end                                  True                     True   \n",
       "Is_month_start                               False                    False   \n",
       "Is_quarter_end                               False                    False   \n",
       "Is_quarter_start                             False                    False   \n",
       "Is_year_end                                  False                    False   \n",
       "Is_year_start                                False                    False   \n",
       "Elapsed                                 1438300800               1438300800   \n",
       "StoreType                                        a                        c   \n",
       "Assortment                                       a                        c   \n",
       "CompetitionDistance                          14130                      620   \n",
       "CompetitionOpenSinceMonth                       12                        9   \n",
       "CompetitionOpenSinceYear                      2006                     2009   \n",
       "Promo2                                           1                        0   \n",
       "Promo2SinceWeek                                 14                        1   \n",
       "Promo2SinceYear                               2011                     1900   \n",
       "PromoInterval                      Jan,Apr,Jul,Oct                     None   \n",
       "State                                           NW                       BE   \n",
       "file                                Rossmann_DE_NW           Rossmann_DE_BE   \n",
       "week                       2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
       "trend                                           86                       74   \n",
       "Max_TemperatureC                                21                       19   \n",
       "Mean_TemperatureC                               13                       14   \n",
       "Min_TemperatureC                                 6                        9   \n",
       "Dew_PointC                                      10                        9   \n",
       "\n",
       "                                                 4  \n",
       "index                                            4  \n",
       "Store                                            5  \n",
       "DayOfWeek                                        5  \n",
       "Date                           2015-07-31 00:00:00  \n",
       "Sales                                         4822  \n",
       "Customers                                      559  \n",
       "Open                                             1  \n",
       "Promo                                            1  \n",
       "StateHoliday                                 False  \n",
       "SchoolHoliday                                    1  \n",
       "Year                                          2015  \n",
       "Month                                            7  \n",
       "Week                                            31  \n",
       "Day                                             31  \n",
       "Dayofweek                                        4  \n",
       "Dayofyear                                      212  \n",
       "Is_month_end                                  True  \n",
       "Is_month_start                               False  \n",
       "Is_quarter_end                               False  \n",
       "Is_quarter_start                             False  \n",
       "Is_year_end                                  False  \n",
       "Is_year_start                                False  \n",
       "Elapsed                                 1438300800  \n",
       "StoreType                                        a  \n",
       "Assortment                                       a  \n",
       "CompetitionDistance                          29910  \n",
       "CompetitionOpenSinceMonth                        4  \n",
       "CompetitionOpenSinceYear                      2015  \n",
       "Promo2                                           0  \n",
       "Promo2SinceWeek                                  1  \n",
       "Promo2SinceYear                               1900  \n",
       "PromoInterval                                 None  \n",
       "State                                           SN  \n",
       "file                                Rossmann_DE_SN  \n",
       "week                       2015-08-02 - 2015-08-08  \n",
       "trend                                           82  \n",
       "Max_TemperatureC                                20  \n",
       "Mean_TemperatureC                               15  \n",
       "Min_TemperatureC                                10  \n",
       "Dew_PointC                                       8  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.head().T.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've engineered all our features, we need to convert to input compatible with a neural network.\n",
    "\n",
    "This includes converting categorical variables into contiguous integers or one-hot encodings, normalizing continuous features to standard normal, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features - after feature engineering\n",
    "\n",
    "Note - Jeremy says all of the feature engineering taken directly from the 3rd place winners and he didn't actually create any of it. Now that we've engineered all our features, we need to convert to input compatible with a neural network. \n",
    "* This includes converting \n",
    "* categorical variables into contiguous integers or one-hot encodings, normalizing continuous features to standard normal, etc...\n",
    "\n",
    "\n",
    "### Neural Net Categorical vs. Continuous Variables\n",
    "\n",
    "* Numbers like dates etc we could treate them as numbers. We will see how to treat them as categorical. Categorical tells NN that for each year you can treat it differently. If treated like number it has to come up with a function like transform.\n",
    "\n",
    "* Cardinality - the number of levels of a categroical variable. \n",
    "\n",
    "* Choosing categorical vs. continuous variable is a modeling decision you get to make. In summary, if it is categorical in the data, it has to be categorical. If it is continuous in the data, you get to pick whether to make it continuous or categorical in the model.\n",
    "\n",
    "* In some respects this is a model decision.\n",
    "Continuous are actually the floating point numbers. They would be very hard to make into a categorical.  \n",
    "\n",
    "* Generally, floating point numbers are hard to make categorical as there are many levels (we call number of levels “Cardinality” — e.g. the cardinality of the day of week variable is 7).\n",
    "\n",
    "### Unknown and Binning\n",
    "Uknown category... pandas has special category unknown, for categories that have not been seen before\n",
    "\n",
    "**Question:** it can be helpful. Do you ever bin continuous variables?[31:02] Jeremy does not bin variables but one thing we could do with, say max temperature, is to group into 0–10, 10–20, 20–30, and call that categorical. Interestingly, a paper just came out last week in which a group of researchers found that sometimes binning can be helpful.\n",
    "\n",
    "\n",
    "**Question:** If you are using year as a category, what happens when a model encounters a year it has never seen before? [31:47] We will get there, but the short answer is that it will be treated as an unknown category. Pandas has a special category called unknown and if it sees a category it has not seen before, it gets treated as unknown.\n",
    "\n",
    "### At this point we have our Categorical and Continuous \n",
    "\n",
    "* Loop through cat_vars and turn applicable data frame columns into categorical columns.\n",
    "* Loop through contin_vars and set them as float32 (32 bit floating point) because that is what PyTorch expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844338"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
    "\n",
    "contin_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
    "\n",
    "n = len(joined); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = 'Sales'\n",
    "joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test[dep] = 0\n",
    "joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_cats(joined_test, joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in contin_vars:\n",
    "    joined[v] = joined[v].fillna(0).astype('float32')\n",
    "    joined_test[v] = joined_test[v].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run on a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "?get_cv_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = get_cv_idxs(n, val_pct=150000/n)\n",
    "joined_samp = joined.iloc[idxs].set_index(\"Date\")\n",
    "samp_size = len(joined_samp); samp_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run on the full dataset, use this instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All - the full dataset, not sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844338"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Uncomment to run on full data set\n",
    "samp_size = n\n",
    "joined_samp = joined.set_index(\"Date\")\n",
    "len(joined_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>CompetitionMonthsOpen</th>\n",
       "      <th>Promo2Weeks</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>...</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>trend</th>\n",
       "      <th>trend_DE</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Store DayOfWeek  Year Month Day StateHoliday CompetitionMonthsOpen  \\\n",
       "Date                                                                            \n",
       "2015-07-31     1         5  2015     7  31        False                    24   \n",
       "2015-07-31     2         5  2015     7  31        False                    24   \n",
       "\n",
       "           Promo2Weeks StoreType Assortment  ... Max_Wind_SpeedKm_h  \\\n",
       "Date                                         ...                      \n",
       "2015-07-31           0         c          a  ...               24.0   \n",
       "2015-07-31          25         a          a  ...               14.0   \n",
       "\n",
       "           Mean_Wind_SpeedKm_h CloudCover trend trend_DE AfterStateHoliday  \\\n",
       "Date                                                                         \n",
       "2015-07-31                11.0        1.0  85.0     83.0              57.0   \n",
       "2015-07-31                11.0        4.0  80.0     83.0              67.0   \n",
       "\n",
       "           BeforeStateHoliday Promo SchoolHoliday Sales  \n",
       "Date                                                     \n",
       "2015-07-31                0.0   1.0           1.0  5263  \n",
       "2015-07-31                0.0   1.0           1.0  6064  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_samp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proc_df\n",
    "\n",
    "Process data frame (fastai). Not DL specific, general purpose tool. \n",
    "\n",
    "1. Pulls out the **dependent variable**, puts it into a separate variable, and deletes it from the original data frame. In other words, df do not have Sales column, and y only contains Sales column. \n",
    "\n",
    "2. `do_scale`: Neural nets really like to have the input data to all be somewhere around zero with a standard deviation of somewhere around 1. So we take our data, subtract the mean, and divide by the standard deviation to make that happen. It returns a special object which keeps track of what mean and standard deviation it used for that normalization so you can do the same to the test set later (mapper).\n",
    "  \n",
    "3. It also handles missing values — for categorical variable, it becomes ID: 0 and other categories become 1, 2, 3, and so on. For continuous variable, it replaces the missing value with the median and create a new boolean column that says whether it was missing or not.  NN like mean to be 0 and std dev of 1. Subtract mean and divide by std dev  \n",
    "   * Categorical values* -  missing values become 0 and others 1, 2, 3, ..   \n",
    "   * Continuous values* - fill with mean. Generate an nother column that tells if missing true/false \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)\n",
    "yl = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test = joined_test.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'],\n",
    "                                  mapper=mapper, na_dict=nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Sales'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(joined_samp.columns), len(df.columns))\n",
    "set(joined_samp.columns)-set(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated dataframe  \n",
    "\n",
    "* Categorical variables year replaced with contiguous integers starting with 0, for example look at \"Year\" columnSimilarly, assortment also has numerical values instead of \"a\" ... \"c\" . Everything is a number. This is exactly what we put into RandomForest\n",
    "* After processing, year 2014 for example becomes 2 since categorical variables have been replaced with contiguous integers starting at zero. The reason for that is, we are going to put them into a matrix later, and we would not want the matrix to be 2014 rows long when it could just be two rows.\n",
    "* Now we have a data frame which does not contain the dependent variable and where everything is a number. That is where we need to get to to do deep learning. Check out Machine Learning course on further details. Another thing that is covered in Machine Learning course is validation sets. In this case, we need to predict the next two weeks of sales therefore we should create a validation set which is the last two weeks of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>CompetitionMonthsOpen</th>\n",
       "      <th>Promo2Weeks</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>...</th>\n",
       "      <th>Min_Humidity</th>\n",
       "      <th>Max_Wind_SpeedKm_h</th>\n",
       "      <th>Mean_Wind_SpeedKm_h</th>\n",
       "      <th>CloudCover</th>\n",
       "      <th>trend</th>\n",
       "      <th>trend_DE</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.620066</td>\n",
       "      <td>0.149027</td>\n",
       "      <td>-0.142774</td>\n",
       "      <td>-1.844823</td>\n",
       "      <td>1.732493</td>\n",
       "      <td>1.724336</td>\n",
       "      <td>0.604460</td>\n",
       "      <td>1.13112</td>\n",
       "      <td>1.113717</td>\n",
       "      <td>2.04105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.264031</td>\n",
       "      <td>-0.960613</td>\n",
       "      <td>-0.142774</td>\n",
       "      <td>-0.488722</td>\n",
       "      <td>1.294579</td>\n",
       "      <td>1.724336</td>\n",
       "      <td>0.926957</td>\n",
       "      <td>1.13112</td>\n",
       "      <td>1.113717</td>\n",
       "      <td>2.04105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Store  DayOfWeek  Year  Month  Day  StateHoliday  \\\n",
       "Date                                                           \n",
       "2015-07-31      1          5     3      7   31             1   \n",
       "2015-07-31      2          5     3      7   31             1   \n",
       "\n",
       "            CompetitionMonthsOpen  Promo2Weeks  StoreType  Assortment  ...  \\\n",
       "Date                                                                   ...   \n",
       "2015-07-31                     25            1          3           1  ...   \n",
       "2015-07-31                     25           26          1           1  ...   \n",
       "\n",
       "            Min_Humidity  Max_Wind_SpeedKm_h  Mean_Wind_SpeedKm_h  CloudCover  \\\n",
       "Date                                                                            \n",
       "2015-07-31     -1.620066            0.149027            -0.142774   -1.844823   \n",
       "2015-07-31     -1.264031           -0.960613            -0.142774   -0.488722   \n",
       "\n",
       "               trend  trend_DE  AfterStateHoliday  BeforeStateHoliday  \\\n",
       "Date                                                                    \n",
       "2015-07-31  1.732493  1.724336           0.604460             1.13112   \n",
       "2015-07-31  1.294579  1.724336           0.926957             1.13112   \n",
       "\n",
       "               Promo  SchoolHoliday  \n",
       "Date                                 \n",
       "2015-07-31  1.113717        2.04105  \n",
       "2015-07-31  1.113717        2.04105  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After processing, year 2014 for example becomes 2 since categorical variables have been replaced with contiguous integers starting at zero. The reason for that is, we are going to put them into a matrix later, and we would not want the matrix to be 2014 rows long when it could just be two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time series data, cross-validation is not random. Instead, our holdout data is generally the most recent data, as it would be in real application. This issue is discussed in detail in [this post](http://www.fast.ai/2017/11/13/validation-sets/) on our web site.\n",
    "\n",
    "One approach is to take the last 25% of rows (sorted by date) as our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Sets\n",
    "\n",
    "**Article - validation sets**  \n",
    "How (and why) to create a good validation set  \n",
    "https://www.fast.ai/2017/11/13/validation-sets/ \n",
    "\n",
    "Predict next two weeks of sales. Therefore validation set has last two weeks. The most similary \n",
    "\n",
    "In time series data, cross-validation is not random. Instead, our holdout data is generally the most recent data, as it would be in real application. This issue is discussed in detail in [this post](http://www.fast.ai/2017/11/13/validation-sets/) on our web site.\n",
    "\n",
    "One approach is to take the last 25% of rows (sorted by date) as our validation set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have a data frame which does not contain the dependent variable and where everything is a number. That is where we need to get to to do deep learning. Check out Machine Learning course on further details. Another thing that is covered in Machine Learning course is validation sets. In this case, we need to predict the next two weeks of sales therefore we should create a validation set which is the last two weeks of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "# train_ratio = 0.9\n",
    "train_size = int(samp_size * train_ratio); train_size\n",
    "val_idx = list(range(train_size, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even better option for picking a validation set is using the exact same length of time period as the test set uses - this is implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = np.flatnonzero(\n",
    "    (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this???\n",
    "#val_idx=[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to put together our models.\n",
    "\n",
    "Root-mean-squared percent error is the metric Kaggle used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_y(a): return np.exp(a)\n",
    "\n",
    "def exp_rmspe(y_pred, targ):\n",
    "    targ = inv_y(targ)\n",
    "    pct_var = (targ - inv_y(y_pred))/targ\n",
    "    return math.sqrt((pct_var**2).mean())\n",
    "\n",
    "max_log_y = np.max(yl)\n",
    "y_range = (0, max_log_y*1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a ModelData object directly from out data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "        displayAlign: 'left'\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = 'my_css.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to put together our models.\n",
    "\n",
    "\n",
    "### RMSPE\n",
    "\n",
    "Root-mean-squared percent error is the metric Kaggle used for this competition. For any Kaggle competitions, it is important that you have a strong understanding of your metric — how you are going to be judged. In this competition, we are going to be judged on Root Mean Square Percentage Error (RMSPE).\n",
    "\n",
    "\n",
    "<script>\n",
    "    MathJax.Hub.Config({\n",
    "        displayAlign: 'left'\n",
    "    });\n",
    "</script>\n",
    "\n",
    "$$  \\enspace\\enspace   EQN-1 \\hspace{3em} RMSPE =   \\sqrt{\\frac{1}{n}\\sum_{k=1}^n \\left(   \\frac{y_i - \\hat{y}_i}{y_i} \\right) } $$\n",
    "\n",
    "\n",
    "**Root Mean Squared Percent Error**\n",
    "We don't have RMSPE metric for PyTorch (not hard to create).  Simply take the log, then difference resulting in log (mean percent error)\n",
    "\n",
    "However, rather than a PyTorch function, its easier to use log properties. The prediction, divided by the actual is the thing we care about. Use log properties\n",
    "```\n",
    "           y'/y\n",
    "E.g. Sum   ---- , use property  ln(a'/b') = ln(a') - ln(b')\n",
    "            n\n",
    "```\n",
    "\n",
    "take the log of our data ... getting the RMSE will be RMSPE for free\n",
    "\n",
    "\n",
    "When want to print out our RMSPE need to take exponent of the log RMSPE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data Object\n",
    "We are finally at the deep learning stage. Now we can create a ModelData object directly from our data frame. As usual, the process looks exactly as before\n",
    "\n",
    "  *  First create a model data objcect, with validation, training and optonal test set built into it. As per usual, we will start by creating model data object which has a validation set, training set, and optional test set built into it. \n",
    "  * From that, we will get a learner, we will then optionally call lr_find, then call learn.fit and so forth. Get learner\n",
    "  * optionally learn.lr_find\n",
    "  * learner.fit \n",
    "\n",
    "  * The difference is, we will not have image classifier data. We have rows and col data. We will use ColumnarModelData.from_data_frame, which will return object with API you are familiar with \n",
    "\n",
    "**Inputs** \n",
    "   - Path ... where should it store model files\n",
    "   - val_idx ... list of indexes for validation set\n",
    "   - df: dataframe  ..independent variables\n",
    "   - yl = np.log(y) ... y came out of proc_df ... see above, dependent variable\n",
    "   - cat_flds: Categorical vartiables - tell it which things do we want to treat as categories ... pass in the list of names. Could do everthing as continuous, they're all numbers, but would be meaningliess. Pass in list of names. \n",
    "   - Other familiar paramters, for example .also set bs ... same as you are used to.\n",
    "\n",
    "**Data Model** . \n",
    "Now have standard model data attribute that you are familiar with, for example  \n",
    "   - classes classes names\n",
    "   - val_idx class labels for validation data\n",
    "   - trn_y class labels for training data\n",
    "   - val_ds validation dataset\n",
    "   - trn_ds training dataset\n",
    "   - test_ds test dataset\n",
    "  \n",
    "   ... exactly as in image based data objects\n",
    "\n",
    "all the same stuff as image data objects\n",
    "\n",
    "Now need to creat model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ColumnarModelData.from_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128,\n",
    "                                       test_df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categorical variables have a lot more levels than others. Store, in particular, has over a thousand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sz = [(c, len(joined_samp[c].cat.categories)+1) for c in cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Store', 1116),\n",
       " ('DayOfWeek', 8),\n",
       " ('Year', 4),\n",
       " ('Month', 13),\n",
       " ('Day', 32),\n",
       " ('StateHoliday', 3),\n",
       " ('CompetitionMonthsOpen', 26),\n",
       " ('Promo2Weeks', 27),\n",
       " ('StoreType', 5),\n",
       " ('Assortment', 4),\n",
       " ('PromoInterval', 4),\n",
       " ('CompetitionOpenSinceYear', 24),\n",
       " ('Promo2SinceYear', 9),\n",
       " ('State', 13),\n",
       " ('Week', 53),\n",
       " ('Events', 22),\n",
       " ('Promo_fw', 7),\n",
       " ('Promo_bw', 7),\n",
       " ('StateHoliday_fw', 4),\n",
       " ('StateHoliday_bw', 4),\n",
       " ('SchoolHoliday_fw', 9),\n",
       " ('SchoolHoliday_bw', 9)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *cardinality* of each variable (that is, its number of unique values) to decide how large to make its *embeddings*. Each level will be associated with a vector with length defined as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1116, 50),\n",
       " (8, 4),\n",
       " (4, 2),\n",
       " (13, 7),\n",
       " (32, 16),\n",
       " (3, 2),\n",
       " (26, 13),\n",
       " (27, 14),\n",
       " (5, 3),\n",
       " (4, 2),\n",
       " (4, 2),\n",
       " (24, 12),\n",
       " (9, 5),\n",
       " (13, 7),\n",
       " (53, 27),\n",
       " (22, 11),\n",
       " (7, 4),\n",
       " (7, 4),\n",
       " (4, 2),\n",
       " (4, 2),\n",
       " (9, 5),\n",
       " (9, 5)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "### embeddings\n",
    "\n",
    "The idea of an embedding is what is called a “distributed representation” — the most fundamental concept of neural networks. This is the idea that a concept in neural network has a high dimensional representation which can be hard to interpret. These numbers in this vector does not even have to have just one meaning. It could mean one thing if this is low and that one is high, and something else if that one is high and that one is low because it is going through this rich nonlinear function. It is this rich representation that allows it to learn such interesting relationships.\n",
    "\n",
    "### Question\n",
    "Are embeddings suitable for certain types of variables? [01:02:45] Embedding is suitable for any categorical variables. The only thing it cannot work well for would be something with too high cardinality. If you had 600,000 rows and a variable had 600,000 levels, that is just not a useful categorical variable. But in general, the third winner in this competition really decided that everything that was not too high cardinality, they put them all as categorical. The good rule of thumb is if you can make a categorical variable, you may as well because that way it can learn this rich distributed representation; where else if you leave it as continuous, the most it can do is to try and find a single functional form that fits it well.\n",
    "\n",
    "\n",
    "### Dates\n",
    "\n",
    "**Question:** Dates and time as categoricals and how they affects seasonality?\n",
    "\n",
    "Dates covered in ML course.\n",
    "add_datepart function takes dataframe and a date. Removes the column and replaces with lots of columns with useful information like day of month, day of year, is it a holiday, is_month_end, etc (8 rows, see paramters).\n",
    "Embeddings ... We end up with 8 rows 4 column embeddings for day for week. Allows model to create time-series cycle. If someting goes up on Monday down on Wednesday, but only in Berlin. The NN will learn the cycle.\n",
    "Seasonality ... The NN learns the seasonality. You must need to make sure that the varible, cycle indicator, such as Monday and Wednesday is in the training set.\n",
    "Example of thing to think about\n",
    "\n",
    "Holidays\n",
    "Sales of beverages in San Francisco, ball games at ATT park, indicator needs to be there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model (the learner)\n",
    "\n",
    "from our our model data (m d) create a learner that is suitable for it\n",
    "dropout at start   ....04  \n",
    "activations in layer 1000,500   \n",
    "dropout in later layers .001, 0.01 \n",
    "\n",
    "**NN Model** \n",
    "\n",
    "```\n",
    " Note: NN matrix products input rows matches next layer columns\n",
    "  - you decide on output cols (your design) for each layer\n",
    "  - input rows for each layer matches output cols from previous layer\n",
    "  \n",
    "  \n",
    "  \n",
    "             Layer N                                    Layer N+1\n",
    "             matrix          output                      input 100 x 50 activations   output\n",
    "           20 inputs (rows) into 100 activations (columns)  \n",
    "           20 x 1000   rank 1 tensor                          _      _\n",
    "20 inputs   _      _                                         |        |\n",
    " in        |        |                RELU                    |        |              softmax\n",
    "     => 20 |        | ->  1 [     ]   ==> [1 x 1000] -> 1000 |        | -> 1 [     ]  =>      \n",
    "1 x N      |_      _|        *1000                      **   |        |         50 \n",
    "Tensor       100            *you pick                        |_      _|\n",
    "    rows match cols (previous layer)                             50\n",
    "                                                      \n",
    "                                                    \n",
    "               Last Layer\n",
    "                  matrix\n",
    "                  50 x 1\n",
    " Softmax            _   \n",
    " Probabilities     | |   1 x 1\n",
    "            =>  50 | | -> [ ]   output\n",
    "                   |_|    \n",
    "                    1\n",
    "                    \n",
    "    no RELU before softmax\n",
    "    Softmax needs negatives \n",
    "    \n",
    "    Note, in the case of regression, prediction of sales (not classification)\n",
    "          do not even neeed the softmax layer\n",
    "    \n",
    "                                         \n",
    " ```\n",
    "\n",
    "Forget about categorical variables for a moment and just think about continuous variables. For continuous variables, for example, say Here's a big list: min temp, max temp, distance to nearest competitor. It will take this 1-D array (vector, rank 1 tensor) and put it through a matrix multiplication. Say there are 20 continuous variables. \n",
    "\n",
    "The NN will take this 1-D array put through matrix multiplication, say 20 continuous variables, a matrix with 20 rows. We decide on how many columns, say 100. It will spit out a new rank length 100 rank 1 tensor (1 row). That is what a matrix product does and definition of a linear layer in DL. Then, put it through RELU (i.e., through away negatives) and then anothr matrix product, will require 100 rows (to match previous columns) to predict 1 column (a single number).\n",
    "\n",
    "<img src=\"./Lesson4Whiteboard1.png\" style='width:300px;height:300px;'>\n",
    "\n",
    "Actually, you would never put RELU in the last layer (that was just a quick and dirty example). Softmax needs negatives to predict probabilieis. \n",
    "\n",
    "In practice, for example, may have 50 columns output from the RELU.  One value to predict sales.and then into final 50 x 1 to spit out single sales prediction. \n",
    "\n",
    "<img src=\"./BasicNNArchitecture.png\" style='width:600px;height:250px;'>\n",
    "\n",
    "Basic NN Architecture - simple view of fully connected NN \n",
    "- input Rank 1 tensor ... lin layer ... activ layer ...  lin layer ... s/max  ... output\n",
    "- decisions to make: could add more liniear layers, could add drop out\n",
    "- Not much crazy architecture to do\n",
    "- Fully connected layers fairly simple, activation funcrtions andd s/max\n",
    "- In this case we have a regression, not even s/max just through away thte s/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Embedding-1',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 50]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 55800)])),\n",
       "             ('Embedding-2',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 4]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 32)])),\n",
       "             ('Embedding-3',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-4',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 91)])),\n",
       "             ('Embedding-5',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 16]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('Embedding-6',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-7',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 338)])),\n",
       "             ('Embedding-8',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 14]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 378)])),\n",
       "             ('Embedding-9',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 3]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 15)])),\n",
       "             ('Embedding-10',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-11',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-12',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 288)])),\n",
       "             ('Embedding-13',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 5]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 45)])),\n",
       "             ('Embedding-14',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 91)])),\n",
       "             ('Embedding-15',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 27]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1431)])),\n",
       "             ('Embedding-16',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 11]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 242)])),\n",
       "             ('Embedding-17',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 4]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 28)])),\n",
       "             ('Embedding-18',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 4]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 28)])),\n",
       "             ('Embedding-19',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-20',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-21',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 5]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 45)])),\n",
       "             ('Embedding-22',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 5]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 45)])),\n",
       "             ('Dropout-23',\n",
       "              OrderedDict([('input_shape', [-1, 199]),\n",
       "                           ('output_shape', [-1, 199]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-24',\n",
       "              OrderedDict([('input_shape', [-1, 16]),\n",
       "                           ('output_shape', [-1, 16]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 32)])),\n",
       "             ('Linear-25',\n",
       "              OrderedDict([('input_shape', [-1, 215]),\n",
       "                           ('output_shape', [-1, 1000]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 216000)])),\n",
       "             ('Dropout-26',\n",
       "              OrderedDict([('input_shape', [-1, 1000]),\n",
       "                           ('output_shape', [-1, 1000]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-27',\n",
       "              OrderedDict([('input_shape', [-1, 1000]),\n",
       "                           ('output_shape', [-1, 500]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 500500)])),\n",
       "             ('Dropout-28',\n",
       "              OrderedDict([('input_shape', [-1, 500]),\n",
       "                           ('output_shape', [-1, 500]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-29',\n",
       "              OrderedDict([('input_shape', [-1, 500]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 501)]))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* emb_szs : embedding size\n",
    "* len(df.columns)-len(cat_vars) : number of continuous variables in the data frame\n",
    "* 0.04 : embedding matrix has its own dropout and this is the dropout rate\n",
    "* 1 : how many outputs we want to create (output of the last linear layer)\n",
    "* [1000, 500] : number of activations in the first linear layer, and the second linear layer\n",
    "* [0.001, 0.01] : dropout in the first linear layer, and the second linear layer\n",
    "* y_range : we will not worry about that for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e61849907246b6ac15b4f6998ed6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3826/6297 [00:44<00:30, 80.80it/s, loss=0.0442] "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "m.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.sched.plot(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Data Set (run this section only with sampled dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "#  Pass in No. of continuous variables =  total Variables - Categoricals. NN knows how to create continuous and cat variables\n",
    "#  Categorical variable dropouts ... .04\n",
    "#  Output of last linear layer ... 1, predicting single number sales\n",
    "#  Activations in first and second linear layers ... [1000,500]\n",
    "#  Dropouts in first and second linear layers ... [.001,0.1]\n",
    "\n",
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4766203150b4cf89268acce96827b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                       \n",
      "    0      0.027075   0.026772   0.168136  \n",
      "    1      0.020292   0.042733   0.178886                        \n",
      "    2      0.018082   0.033591   0.161527                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.03359]), 0.16152679955443033]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 3, metrics=[exp_rmspe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79a9b6b03814beea38cae2c3d1a697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                       \n",
      "    0      0.011886   0.016945   0.120789  \n",
      "    1      0.011497   0.016056   0.117751                        \n",
      "    2      0.01111    0.014842   0.114942                        \n",
      "    3      0.010293   0.014557   0.113945                         \n",
      "    4      0.009217   0.014033   0.112515                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.01403]), 0.11251475376501577]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 5, metrics=[exp_rmspe], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9cbeac4a4f4d5ca949253a303bf3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                       \n",
      "    0      0.011715   0.018787   0.149207  \n",
      "    1      0.009612   0.014667   0.114463                         \n",
      "    2      0.008655   0.01321    0.110659                         \n",
      "    3      0.006944   0.013313   0.110169                         \n",
      "    4      0.010628   0.014148   0.122925                        \n",
      "    5      0.008509   0.013768   0.110914                         \n",
      "    6      0.007199   0.0136     0.110304                         \n",
      "    7      0.006271   0.012801   0.108757                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.0128]), 0.10875654820105955]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 2, metrics=[exp_rmspe], cycle_len=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All (Learn on the entire data set , not sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3668f521b0ca4af39deb9af88be1d79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                       \n",
      "    0      0.014241   0.015422   0.124388  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.01542]), 0.12438843071519944]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 1, metrics=[exp_rmspe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc47721aa8bd40f38e9e48322d21e392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                       \n",
      "    0      0.011781   0.012708   0.107947  \n",
      "    1      0.009749   0.013042   0.105729                         \n",
      "    2      0.009098   0.011467   0.101354                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.01147]), 0.10135394351206994]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 3, metrics=[exp_rmspe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd40a35f6d840eabf194032feec2cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                        \n",
      "    0      0.007065   0.010563   0.097754  \n",
      "    1      0.006993   0.010614   0.097826                         \n",
      "    2      0.007411   0.010584   0.097618                         \n",
      "    3      0.006514   0.010569   0.0975                           \n",
      "    4      0.006186   0.010604   0.097359                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.0106]), 0.09735854323381407]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 5, metrics=[exp_rmspe], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0177d03e6a045baa45af1ffc277ec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   exp_rmspe                        \n",
      "    0      0.00653    0.010652   0.097382  \n",
      "    1      0.006001   0.01058    0.097255                         \n",
      "    2      0.006395   0.010368   0.096616                         \n",
      "    3      0.006307   0.010474   0.096289                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.01047]), 0.09628930141547118]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(lr, 4, metrics=[exp_rmspe], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('val0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics : this is a custom metric which specifies a function to be called at the end of every epoch and prints out a result\n",
    "\n",
    "By using all of the training data, we achieved a RMSPE around 0.08757 (Jeremy's result, presented in class). There is a big difference between public leader board and private leader board, but we are certainly in the top end of this competition.\n",
    "\n",
    "Folks at Pinterest who build a very similar model for recommendations also said that when they switched from gradient boosting machines to deep learning, they did way less feature engineering and it was much simpler model which requires less maintenance. So this is one of the big benefits of using this approach to deep learning — you can get state of the art results but with a lot less work\n",
    "\n",
    "So this is a technique for dealing with time series and structured data. Interestingly, compared to the group that used this technique (Entity Embeddings of Categorical Variables), the second place winner did way more feature engineering. The winners of this competition were actually subject matter experts in logistics sales forecasting so they had their own code to create lots and lots of features. Folks at Pinterest who build a very similar model for recommendations also said that when they switched from gradient boosting machines to deep learning, they did way less feature engineering and it was much simpler model which requires less maintenance. So this is one of the big benefits of using this approach to deep learning — you can get state of the art results but with a lot less work.\n",
    "\n",
    "Question: Are we using any time series in any of these? [01:15:01] Indirectly, yes. As we just saw, we have a day of week, month of year, etc in our columns and most of them are being treated as categories, so we are building a distributed representation of January, Sunday, and so on. We are not using any classic time series techniques, all we are doing is true fully connected layers in a neural net. The embedding matrix is able to deal with things like day of week periodicity in a much richer way than than any standard time series techniques.\n",
    "\n",
    "Question regarding the difference between image models and this model [01:15:59]: There is a difference in a way we are calling get_learner. In imaging we just did Learner.trained and pass the data:\n",
    "\n",
    "For these kinds of models, in fact for a lot of the models, the model we build depends on the data. In this case, we need to know what embedding matrices we have. So in this case, the data objects creates the learner (upside down to what we have seen before):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology Summary\n",
    "\n",
    "Step 1   \n",
    "- list the categorical variable names\n",
    "- list continuouis variable names\n",
    "- Put in pandas dataframe  \n",
    "\n",
    "Step 2 - list of row index in validation set ... val_idx  \n",
    "Step 3 - call ColumnarModelData.from_data_frame  \n",
    "Step 4 - embeddings matrix  \n",
    "Step 5 - call md.get_learner() ... you can use the exact parameters above fiddle if over or under fits then   fiddle with parameters\n",
    "Step 6 - call m.fit ... \n",
    "\n",
    "### Discussion \n",
    "\n",
    "\n",
    "What is drop out doing in this case\n",
    "- output of linear layer is rank 1 tensor ... e.g., drop 50%\n",
    "- similar with embeddings vector ... e.g., drop 50%\n",
    "\n",
    "What's the downside. Almost know one using this, why not. \n",
    "\n",
    "**Question:** How to use data augmentation for this type of data, and how does dropout work? [01:18:59] No idea. Jeremy thinks it has to be domain-specific, but he has never seen any paper or anybody in industry doing data augmentation with structured data and deep learning. He thinks it can be done but has not seen it done. What dropout is doing is exactly the same as before.\n",
    "\n",
    "\n",
    "Data Augmentation\n",
    " - will be domain specific. Have not seen it done in the industry with structured data\n",
    "\n",
    "#### What's the benefit of DL on Structured Data\n",
    "- Academia not doing much in structured data, because academia not interested in publishing\n",
    "- Prior to Fastai, you had to write all the custom code. \n",
    "- With Fastai now you can essentially do it with 6 lines of code\n",
    "- Should be new opportunities to solve problems that had been previously done with ML\n",
    "- Jeremy working on structured DL models for about 1 year. This class is the first time there is half a dozen people in the world using it. \n",
    "- There is a post instacart on what they are doing\n",
    "- O'Reilly AI video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (run this on the test data set ... on either sampled or all data set)\n",
    "\n",
    "- Load the model and predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ... define the model, load, and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128,\n",
    "                                       test_df=df_test)\n",
    "\n",
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load('val0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=m.predict_with_targs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09997347595447578"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_rmspe(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=m.predict(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.exp(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test['Sales']=pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fn=f'{PATH}tmp/sub.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test[['Id','Sales']].to_csv(csv_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/rossmann/tmp/sub.csv' target='_blank'>data/rossmann/tmp/sub.csv</a><br>"
      ],
      "text/plain": [
       "/home/paperspace/fastai/courses/dl1/data/rossmann/tmp/sub.csv"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(csv_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4504.1655],\n",
       "       [ 7509.4805],\n",
       "       [ 9319.568 ],\n",
       "       [ 7566.9277],\n",
       "       [ 7453.5645],\n",
       "       [ 6075.2817],\n",
       "       [ 7460.306 ],\n",
       "       [ 8097.3286],\n",
       "       [ 5260.166 ],\n",
       "       [ 5864.3965],\n",
       "       [ 7239.9814],\n",
       "       [ 8378.206 ],\n",
       "       [ 7408.302 ],\n",
       "       [ 9553.477 ],\n",
       "       [ 5921.4097],\n",
       "       [ 5163.6865],\n",
       "       [ 5904.2617],\n",
       "       [10052.505 ],\n",
       "       [10589.701 ],\n",
       "       [ 9904.076 ],\n",
       "       ...,\n",
       "       [ 7281.3403],\n",
       "       [13741.914 ],\n",
       "       [ 6217.177 ],\n",
       "       [ 5366.7993],\n",
       "       [ 7806.3457],\n",
       "       [ 8492.542 ],\n",
       "       [ 3249.8674],\n",
       "       [ 8892.701 ],\n",
       "       [ 7109.9014],\n",
       "       [ 5303.055 ],\n",
       "       [ 6111.211 ],\n",
       "       [ 4994.694 ],\n",
       "       [ 2966.486 ],\n",
       "       [ 5946.6777],\n",
       "       [ 3911.0332],\n",
       "       [ 3011.966 ],\n",
       "       [ 7573.714 ],\n",
       "       [ 5957.5024],\n",
       "       [24057.578 ],\n",
       "       [ 6723.2686]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128,\n",
    "                                       test_df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load('val0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4504.1655],\n",
       "       [ 7509.4805],\n",
       "       [ 9319.568 ],\n",
       "       [ 7566.9277],\n",
       "       [ 7453.5645],\n",
       "       [ 6075.2817],\n",
       "       [ 7460.306 ],\n",
       "       [ 8097.3286],\n",
       "       [ 5260.166 ],\n",
       "       [ 5864.3965],\n",
       "       [ 7239.9814],\n",
       "       [ 8378.206 ],\n",
       "       [ 7408.302 ],\n",
       "       [ 9553.477 ],\n",
       "       [ 5921.4097],\n",
       "       [ 5163.6865],\n",
       "       [ 5904.2617],\n",
       "       [10052.505 ],\n",
       "       [10589.701 ],\n",
       "       [ 9904.076 ],\n",
       "       ...,\n",
       "       [ 7281.3403],\n",
       "       [13741.914 ],\n",
       "       [ 6217.177 ],\n",
       "       [ 5366.7993],\n",
       "       [ 7806.3457],\n",
       "       [ 8492.542 ],\n",
       "       [ 3249.8674],\n",
       "       [ 8892.701 ],\n",
       "       [ 7109.9014],\n",
       "       [ 5303.055 ],\n",
       "       [ 6111.211 ],\n",
       "       [ 4994.694 ],\n",
       "       [ 2966.486 ],\n",
       "       [ 5946.6777],\n",
       "       [ 3911.0332],\n",
       "       [ 3011.966 ],\n",
       "       [ 7573.714 ],\n",
       "       [ 5957.5024],\n",
       "       [24057.578 ],\n",
       "       [ 6723.2686]], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on notes from Anze below\n",
    "\n",
    "# create ColumnarDataSet from DataFrame (df)\n",
    "dummy_y=len(df_test)*[0]\n",
    "cds = ColumnarDataset.from_data_frame(df_test,cat_vars)\n",
    "\n",
    "dl = DataLoader(cds,batch_size=128)\n",
    "\n",
    "predictions = m.predict_dl(dl)\n",
    "predictions=np.exp(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction  ... options for prediction based on notes from fastai forum\n",
    "https://forums.fast.ai/t/understanding-columnarmodeldata-from-data-frame-from-rossman/8140/4\n",
    "\n",
    "My workflow is very usual:\n",
    "\n",
    "train NN on columnar data  \n",
    "\n",
    "a)  load train/val set  \n",
    "b) do ‘preprocessing’ (i.e. standardize each column)  \n",
    "c) initialize ColumnarModelData and get the learner  \n",
    "d) use lr_find; SGDR;  \n",
    "e) save model at the end\n",
    "  \n",
    "\n",
    "go to local machine  \n",
    "a) load sample  \n",
    "b) do 'preprocessing’  \n",
    "c) initialize ColumnarModelData using loaded sample and get the learner  \n",
    "d) load model trained on AWS  \n",
    "e) do classification on loaded sample    \n",
    "\n",
    "In order to do step 2b) I need the mapper from step 1b) and pass it as an argument in proc_df function, right? So I need to serialize the mapper as well.  \n",
    "\n",
    "Is step 2c correct?  \n",
    "\n",
    "\n",
    "Yeap, for step 2c, you would need to pass in the sample data from proc_df into ColumnarModelData. After that, get learner, load the model, and run the prediction.\n",
    "\n",
    "To predict the test sets, I have used the workaround steps below, worked fine for me so far.\n",
    "http://forums.fast.ai/t/structured-learner/8224/3\n",
    "\n",
    "\n",
    "What I am doing for now as a temp solution is (at least works):\n",
    "\n",
    "### kcturgutflu\n",
    "Tune model with train and val sets.\n",
    "* Store these hyperparameters (lr, dimension of embeddings)\n",
    "* Combine train and test (add target column to test)\n",
    "* Run model as you are running with val\n",
    "* Make predictions with m.predict()\n",
    "\n",
    "Note: I am very fascinated by expressing categorical variables in Eucledian spaces, especially after seeing some t-SNE visualizations on them. Very cool :smiley:\n",
    "\n",
    "\n",
    "### Weather and tred corresponding to test\n",
    "As for weather and googletrend data, I think we should have it since it’s the only way 3rd place winners to had a submission. So it must be somewhere in there\n",
    "\n",
    "You should be able to grab it here https://github.com/entron/entity-embedding-rossmann/tree/kaggle\n",
    "\n",
    "\n",
    "### Anze ... there’s another solution:\n",
    "\n",
    "create ColumnarDataSet from DataFrame (df)  \n",
    "cds = ColumnarDataset.from_data_frame(df,cat_flds=[…put your cat vars here…],y=dummy_y)\n",
    "\n",
    "create DataLoader from ColumnarDataSet  \n",
    "dl = DataLoader(cds)  \n",
    "\n",
    "make predictions for DataLoader  \n",
    "predictions = m.predict_dl(dl)  \n",
    "\n",
    "### arjunrajkumar ... Hi Anze,\n",
    "\n",
    "Just a little lost here. How do you pass the test data while using these three steps?  \n",
    "Do I have to create a new df containing only the test sets using the proc_df command, \n",
    "and then make sure that that df should have the same columns as the ones on training?  \n",
    "\n",
    "### Anze Zupanc\n",
    "@arjunrajkumar Exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "((val,trn), (y_val,y_trn)) = split_by_idx(val_idx, df.values, yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrf = RandomForestRegressor(n_estimators=40, max_features=0.99, min_samples_leaf=2,\n",
    "                          n_jobs=-1, oob_score=True)\n",
    "mrf.fit(trn, y_trn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9822924861623946,\n",
       " 0.931704191178399,\n",
       " 0.9247724900452019,\n",
       " 0.10854763755522956)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mrf.predict(val)\n",
    "mrf.score(trn, y_trn), m.score(val, y_val), m.oob_score_, exp_rmspe(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rossman test DL methodology\n",
    "\n",
    "Look at Rossman added test set ... all lines should now contain test comparisons\n",
    "\n",
    "Understand how well the method works ... \n",
    "\n",
    "- Code does not run top to bottom \n",
    "\n",
    "- apply_cats rather than test_cats to make sure same categorical codes\n",
    "\n",
    "- create model data object pass in the test data\n",
    "\n",
    "**Public and Private Kaggle Scores**  \n",
    "When finish trining it then call predict, say True to say test set\n",
    "\n",
    "Then pass to Kaggle \n",
    "\n",
    "Public score 0.103 ... 300th place looks awful \n",
    "Privat score 0.107 ... 5th \n",
    "\n",
    "If you are competing and have not thoughtfully created validation set of your own and relying on public leaderboard , you may look good in public, but in private not look so good. \n",
    "\n",
    "\n",
    "**Iceberg competition**  \n",
    "\n",
    "large percentage of public leader board synthetically generated data augmentation data.\n",
    "\n",
    "your validation set should be very helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rossman test DL methodology\n",
    "\n",
    "There are some nice Kernals in Rossman. Especially if you are doing the ecuador grocery competiton, some of these will be helpful. \n",
    "\n",
    "<img src=\"./RossmanSundays.png\" style='width:300 px;height:300 px;'>\n",
    "\n",
    "One of them showed for sales of Sundays and non-Sundays do and don't look similar.\n",
    "\n",
    "<img src=\"./RossmanStoreClosures.png\" style='width:300 px;height:300 px;'>\n",
    "\n",
    "Rossman 3rd place winnersRossman, whose approach we used. Cool vilsualization showing that just after or before the store closes there are very high demand. The 3rd place winner deleted store close dates. Surely they would have won otherwise. There model was trying to fit to these extremes, that it had no predictive data for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking through Fastai Code\n",
    "\n",
    "**Get_learner**  \n",
    "\n",
    "\n",
    "... got the model, find out what kind of model\n",
    "   ```\n",
    "  ../fastai/courses/dl1/column_data.py\n",
    "  \n",
    "   uses PyTorch mixedInputModel wraps it in the Fastai Structured learner which wraps the model and the data together\n",
    "   \n",
    "   MixedInputModel\n",
    "   ```\n",
    "\n",
    "**Note MixedModel (Torch)**  \n",
    "- always expects categorical (can be empty list) and continuous and will work\n",
    "- some of thise hacky cases will be fixed with PyTorch 0.4 which will handle rank0 Tensor \n",
    "\n",
    "**PyTorch registers list**  \n",
    "nn.ModuleList ... register everything in list to be part of the model\n",
    "\n",
    "\n",
    "Don't treat these as black boxes ... play withg them paste into Jupyter notebook, modify and create your own versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
