{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Embeddings in the Rossman Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret embeddings in Rossman Kaggle prediction. Recall that Rosmman prediction is how much are we going to sell at each store on each day\n",
    "\n",
    "Look at paper Guo and Berkhahn - Rossman competition\n",
    "\n",
    "https://arxiv.org/pdf/1604.06737.pdf\n",
    "\n",
    "<img src=\"./EntityEmbeddingLayersGuoBekhan.png\" style='width:400px;height:75 px;'>\n",
    "\n",
    "Consider an Entity Encoding equivalent to a one-hot encoding fultiplied by a matrix multiplication (EE layer A)\n",
    "\n",
    "If you have 3 embeddings (categorical layer), then 3 one-hot encoding and 3 matrix multiplications, then a dense layer (PyTorch calls a linear layer).\n",
    "\n",
    "2nd paper to show categorical embeddings for this kind of data set. Goes through plenty of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComparisonEntityEmbeddings.png\n",
    "\n",
    "<img src=\"./ComparisonEntityEmbeddings.png\" style='width:350px;height:350 px;'>\n",
    "\n",
    "After training NN with the embeddings, what else can we do with them. Got winning result with NN embeddings. So, how about take embeddings and replace the categorical variable with its embedding. Tried this with GBN (Grdient Boosted Machines), Random Forst, KNN, nearly as good as the NN. \n",
    "\n",
    "**Reuse of embeddings**\n",
    "Can train for embedding of stores, product types then everyone else in the org can chuck those into their GBM and use them. You can even use KNN and get nearly as good results.\n",
    "\n",
    "Give the power of NN of everyone in the org the power without everyone having to do DL. \n",
    "\n",
    "**Embeddings in Database**  \n",
    "chuck them into a database and and empower everyone to use these without having to do deep learning.\n",
    "\n",
    "Embeddings in database table\n",
    "\n",
    "Table each product and embedding vector\n",
    "\n",
    "Then do an inner join to grab the embeddings into a traing set or validation set.\n",
    "\n",
    "**Embeddings with GBM or RF**  \n",
    "GBM's and RF learn a lot quicker than NN, so this is very useful idea.\n",
    "\n",
    "Powerful idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot states of germany\n",
    "\n",
    "<img src=\"./RossmanEmbeddingsMapOfGermany.png\" style='width:400 px;height:400 px;'>\n",
    "\n",
    "\n",
    "Guo and Berkhahn plot embeddings \n",
    "\n",
    "States of Germany, plotted first two PCA components\n",
    "\n",
    "Jeremy put circled cities on a map ... essentially the embeddings inadvertently plotted states of Germany, althouh the concept of geometry did not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings\n",
    "\n",
    "Look for things you expect to see and things that surprise you. Try clusterings.\n",
    "\n",
    "Paper looked at stores that are nearby each other pyhsically have similar characteristics\n",
    "\n",
    "<img src=\"./RossmanEmbeddingsStores.png\" style='width:300 px;height:300 px;'>\n",
    "\n",
    "Jermy did the same thing for day of week and months of year\n",
    "\n",
    "<img src=\"./RossmanEmbeddingsDaysMonths.png\" style='width:300 px;height:300 px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Skip Grams\n",
    "\n",
    "Skip grams specific to NLP. May not cover in this course. Word2vec approach to generate embeddings, don't have supervised learning set. So, made up skip grams. \n",
    "\n",
    "**Word2Vec** \n",
    "Built a ML model on it. That become word to Vec. Originially said \"cat\" replaced with \"justice.\" One sentence keep skip gram exactly as is. Keep a copy unchanged, and other that has been changed. Now can build an ML model. \n",
    "\n",
    "So, if you make this as a single matrix multiply, \"shallow learning\" model then it becomes super useful. Embeddings with linear characteristics you  can use for other purposes. For example, can add them together, subtract them. Word2Vec, for example, shallower model not deep. Train on large dataset. Embeddings have linear characteristics, can be add or subtract the embeddings. Draw nicely. K-nearest neighbors. shallow learning is good\n",
    "\n",
    "**NN Embeddings**  \n",
    "NN embeddings are more predictive.\n",
    "On the oterhand, Jeremy pushing idea to go beyond Word2Vec for NLP from linear, Word2Vec because they are less predictive. \n",
    "\n",
    "Train full pre-trainined embeddings based model. \n",
    "\n",
    "We created embeddings for Rossman by predicting store sales. \n",
    "\n",
    "**Supervised, Unsupervised, Fake Tasks** . \n",
    "\n",
    "For any feature space need labeled data or generate some labeled data.\n",
    "\n",
    "Most of the time when people say unsupervised learning most of the time there are fake tasks to emulate supervised learing.\n",
    "\n",
    "**Auto Encoder** \n",
    "\n",
    "Ultimate, fake tasks is the autoencoder. Recently won Kaggle claims prediction.\n",
    "\n",
    "Insurance policies knew how much was claimed\n",
    "\n",
    "Policy --> NN reconstruct itself --> middle layer with fewer activations  -- more activations for reconstruction --> output = input\n",
    "\n",
    "Most uncreative, but works surprisingly well\n",
    "\n",
    "Took the features that it learned and chucked it into another NN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Is language model trained on archived data useful on movie data? Not well known. \n",
    "\n",
    "In computer vision, shockingly affective to train on cats and dogs and then use pre-trainined network for CT scenas  cancer diagnosis\n",
    "\n",
    "NLP researchers have not tried pre-trained networks very much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
